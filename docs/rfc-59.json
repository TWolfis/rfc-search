{
  "series": "Request for Comments",
  "number": "59",
  "howpublished": "RFC 59",
  "publisher": "RFC Editor",
  "doi": "10.17487/RFC0059",
  "url": "https://www.rfc-editor.org/info/rfc59",
  "title": "Flow Control - Fixed Versus Demand Allocation",
  "pagetotal": "7",
  "year": "1970",
  "month": "jun",
  "body": "\n\n\n\n\n\n                          Edwin W. Meyer, Jr.\n                            MIT Project MAC\n                              27 June 1970\n\n\nThe method of flow control described in RFC 54, prior allocation of\nbuffer space by the use of ALL network commands, has one particular\nadvantage. If no more than 100% of an NCP's buffer space is allocated,\nthe situation in which more messages are presented to a HOST then it can\nhandle will never arise.\n\nHowever, this scheme has very serious disadvantages:\n\n\n(i)  chronic underutilization of resources,\n\n(ii) highly restricted bandwidth,\n\n(iii)considerable overhead under normal operation,\n\n(iv) insufficient flexibility under conditions of increasing load,\n\n(v)  it optimizes for the wrong set of conditions, and\n\n(vi) the scheme breaks down because of message length indeterminacy.\n\nSeveral people from Project MAC and Lincoln Laboratories have discussed\nthis topic, and we feel that the \"cease on link\" flow control scheme\nproposed in RFC 35 by UCLA is greatly preferable to this new plan for\nflow control.\n\nThe method of flow control proposed in RFC 46, using BLK and RSM control\nmessages, has been abandoned because it can not guarantee to quench flow\nwithin a limited number of messages.\n\n\nThe advantages of \"cease on link\" to the fixed allocation proposal are\nthat:\n\n(i)  it permits greater utilization of resources,\n\n(ii) does not arbitrarily limit transmission bandwidth,\n\n(iii)is highly flexible under conditions of changing load,\n\n(iv) imposes no overhead on normal operation, and\n\n(v)  optimizes for the situations that most often occur.\n\nIts single disadvantage is that under rare circumstances an NCP's input\nbuffers can become temporarily overloaded. This should not be a serious\ndrawback for network operation.\n\nThe \"cease on link\" method of flow control operates in the following\n\n\n\n                                                                [Page 1]\n\f\nNWG/RFC 59   Flow Control - Fixed Versus Demand Allocation\n\n\nmanner.  IMP messages for a particular \"receive\" link may be coming in\nto the destination HOST faster than the attached process is reading them\nout of the NCP's buffers. At some point the NCP will decide that the\ninput queue for that link is too large in relation to the total amount\nof free NCP buffer space remaining. At this time the NCP initiates\nquenching by sending a \"cease on link\" IMP message to its IMP. This does\nnothing until the next message for that link comes in to the destination\nIMP. The message still gets transmitted to the receiving HOST. However,\nthe RFNM returned to the transmitting HOST has a special bit set. This\nindicates to the originating NCP that it should stop sending over that\nlink. As a way of confirming the suspension, the NCP sends an SPD \u003clink\u003e\n\"suspended\" NCP control message to the receiving HOST, telling it that\nit indeed has stopped transmitting. At a future time the receiving pro-\ncess will have cut the input queue for the link down to reasonable size,\nand the NCP tells the sending NCP to begin sending messages by issuing a\nRSM \u003clink\u003e \"resume\" NCP control message.\n\nThe flow control argument is based on the following premises:\n\n\n(1)  Most network transmission falls into two categories:\n     Type 1 - short messages (\u003c500 bits) at intervals of several seconds\n     or more. (console communication)\n     Type 2 - a limited number (10 - 100) of full messages (8000 bits)\n     in rapid succession. (file transmission)\n\n\n(2)  Most processes are ready to accept transmitted data when it arrives\n     at the destination and will pick it up within a few seconds (longer\n     for large files). Thus, at any particular instant the great major-\n     ity of read links have no data buffered at the destination HOST.\n     This assumes a sensible software system at both ends.\n\n\n(3)  Flow control need be imposed only rarely on links transmitting Type\n     1 messages, somewhat more frequently for Type 2 messages.\n\n\n(4)  Both the total network load and that over a single connection fluc-\n     tuate and can not be adequately predicted by either the NCP or the\n     process attached to an individual connection.\n\n\n(5)  Assuming adequate control of wide bandwidth transmission (Type 2),\n     the probability that an NCP will be unable to accept messages from\n     the IMP due to full buffers is quite small, even if the simultane-\n     ous receipt of moderately small messages over all active links\n     would more than fill the NCP's input buffers.\n\n\n(6)  In the event that an NCP's buffers do fill completely, it may\n     refuse to accept any transmission from the IMP for up to a minute\n     without utter catastrophe.\n\n\n\n\n                                                                [Page 2]\n\f\nNWG/RFC 59   Flow Control - Fixed Versus Demand Allocation\n\n\n(7)  Under cases of extreme input load, if an NCP has large amounts of\n     data buffered for input to a local process, AND that process has\n     not read data over that connection for more than a minute, the NCP\n     may delete that data to make space for messages from the IMP. This\n     is expected to happen extremely rarely, except in cases where the\n     process is the main contributor to the overload by maintaining\n     several high volume connections which it is not reading.\n\n\n\nBased on the preceding premises, I see the following disadvantages in\nthe flow control proposed in RFC 54:\n\n\n(1)  Chronic Underutilization of Resources - A fixed allocation of\n     buffer space dedicated to a single Type 1 connection will go\n     perhaps 90% unused if we actually achieve responsive console\n     interaction through the network. This is because it is empty most\n     of the time and is very seldom more than partially filled. Stated\n     conversely, a scheme of demand allocation might be expected to han-\n     dle several times as many console connections using the same buffer\n     resources. (It has been suggested that this problem of underutili-\n     zation could be alleviated by allocating more than 100% of the\n     available buffer space. This is in fact no solution at all, because\n     it defeats the basic premise underlying this method of flow con-\n     trol: guaranteed receptivity. True, it might fail in less than one\n     case in 1000, but that is exactly the case for which flow control\n     exists.)\n\n\n(2)  Highly Restricted Bandwidth - At the same time that all that buffer\n     space dedicated to low volume connections is being wasted (and it\n     can't be deallocated - see below), high volume communication is\n     unnecessarily slowed because of inadequate buffer allocation.\n     (Because of the inability to deallocate, it is unwise to give a\n     large allocation.) Data is sent down the connection to the alloca-\n     tion limit, then stops. Several seconds later, after the receiving\n     process picks up some of the data, a new allocation is made, and\n     another parcel of data is sent. It seems clear that even under only\n     moderately favorable conditions, a demand allocation scheme will\n     allow several times the bandwidth that this one does.\n\n\n(3)  Considerable Overhead During Normal Operation - It can be seen that\n     flow control actually need be imposed relatively rarely. However,\n     this plan imposes a constant overhead for all connections due to\n     the continuing need to send new allocations. Under demand alloca-\n     tion, only two RFC's, two CLS's, and perhaps a couple of SPD and\n     RSM control messages need be transmitted for a single connection.\n     Under this plan, a large fraction of the NCP control messages will\n     be ALL allocation requests. There will probably be several times as\n     many control messages to be processed by both the sending and\n     receiving NCP's, as under a demand allocation scheme.\n\n\n\n\n                                                                [Page 3]\n\f\nNWG/RFC 59   Flow Control - Fixed Versus Demand Allocation\n\n\n(4)  Inflexibility Under Increasing Load Conditions - Not only is this\n     plan inflexible as to different kinds of load conditions on a sin-\n     gle link, there is potential inflexibility under increasing total\n     load. The key problem here is that an allocation can not be arbi-\n     trarily revoked. It can be taken back only if it is used. As an\n     example of the problem that can be caused, assume the case of a\n     connection made at 9 AM. The HOST controlling the receiving socket\n     senses light load, and gives the connection a moderately large\n     allocation. However, the process attached to the send socket\n     intends to use it only to report certain special events, and\n     doesn't normally intend to send much at all down this connection.\n     Comes 12 noon, and this connection still has 90% of its original\n     allocation left. Many other processes are now using the network,\n     and the NCP would dearly love to reduce its allocation, if only it\n     could. Of course it can't. If the NCP is to keep its part of the\n     flow control bargain, it must keep that space empty waiting for the\n     data it has agreed to receive.\n\n     This problem can't really be solved by basing future allocations on\n     past use of the connection, because future use may not correlate\n     with past use. Also, the introduction of a deallocation command\n     would cause synchrony problems.\n\n     The real implication of this problem is that an NCP must base its\n     allocation to a link on conditions of heavy load, even if there is\n     currently only light network traffic.\n\n\n(5)  Wrong Type of Optimization - This type of flow control optimizes\n     for the case where every connection starts sending large amounts of\n     data almost simultaneously, exactly the case that just about never\n     occurs. As a result the NCP operates almost as slowly under light\n     load as it does under heavy load.\n\n\n(6)  Loss of Allocation Synchrony Due to Message Length Indeterminacy -\n     If this plan is to be workable, the allocation must apply to the\n     entire message, including header, padding, text, and marking, oth-\n     erwise, a plethora of small messages could overflow the buffers,\n     even though the text allocation had not been exceeded. Thomas Bar-\n     kalow of Lincoln Laboratories has pointed out that this also fails,\n     because the sending HOST can not know how many bits of padding that\n     the receiving HOST's system will add to the message. After several\n     messages, the allocation counters of the sending and receiving\n     HOSTs will get seriously out of synchrony. This will have serious\n     consequences.\n\n     It has been argued that the allocation need apply only to the text\n     portion, because the header, padding, and marking are deleted soon\n     after receipt of the message. This imposes an implementation res-\n     triction on the NCP, that it must delete all but the text part of\n     the message just as soon as it gets it from the IMP. In both the\n     TX2 and Multics implementations it is planned to keep most of the\n     message in the buffers until it is read by the receiving process.\n\n\n\n                                                                [Page 4]\n\f\nNWG/RFC 59   Flow Control - Fixed Versus Demand Allocation\n\n\nThe advantages of demand allocation using the \"cease on link\" flow con-\ntrol method are pretty much the converse of the disadvantages of fixed\nallocation. There is much greater resource utilization, flexibility,\nbandwidth, and less overhead, primarily because flow control restric-\ntions are imposed only when needed, not on normal flow.\n\n\nOne would expect very good performance under light and moderate load,\nand I won't belabor this point.\n\n\nThe real test is what happens under heavy load conditions. The chief\ndisadvantage of this demand allocation scheme is that the \"cease on\nlink\" IMP message can not quench flow over a link soon enough to prevent\nan NCP's buffers from filling completely under extreme overload.\n\nThis is true. However, it is not a critical disadvantage for three rea-\nsons:\n\n\n(i)  An overload that would fill an NCP's buffers is expected to occur\n     at infrequent intervals.\n\n\n(ii) When it does occur, the situation is generally self-correcting and\n     lasts for only a few seconds. Flow over individual connections may\n     be temporarily delayed, but this is unlikely to be serious.\n\n\n(iv) In a few of these situations radical action by the NCP may be\n     needed to unblock the logjam. However, this will have serious\n     consequences only for those connections directly responsible for\n     the tie-up.\n\nLet's examine the operation of an NCP employing demand allocation and\nusing \"cease on link\" for flow control. The following discussion is\nbased on a flow control algorithm in which the maximum permissible queue\nlength (MQL) is calculated to be a certain fraction (say 10%) of the\ntotal empty buffer space currently available. The NCP will block a con-\nnection if the input queue length exceeds the MQL. This can happen\neither due to new input to the queue or a new calculation of the MQL at\na lower value. Under light load conditions, the MQL is reasonably high,\nand relatively long input queues can be maintained without the connec-\ntion being blocked.\n\nAs load increases, the remaining available buffer space goes down, and\nthe MQL is constantly recalculated at a lower value. This hardly affects\nconsole communications with short queues, but more queues for high\nvolume connections are going above the MQL. As they do, \"cease on link\"\nmessages are being sent out for these connections.\n\nIf the flow control algorithm constants are set properly, there is a\nhigh probability that flow over the quenched links will indeed cease\nbefore the scarcity of buffer space reaches critical proportions.\n\n\n\n                                                                [Page 5]\n\f\nNWG/RFC 59   Flow Control - Fixed Versus Demand Allocation\n\n\nHowever, there is a finite probability that the data still coming in\nover the quenched links will fill the buffers. This is most likely under\nalready heavy load conditions when previously inactive links start\ntransmitting at at once at high volume.\n\nOnce the NCP's buffers are filled it must stop taking all messages from\nits IMP. This is serious because now the NCP can no longer receive con-\ntrol messages sent by other NCP's, and because the IMP may soon be\nforced to stop accepting messages from the NCP. (Fortunately, the NCP\nalready has sent out \"cease on link\" messages for virtually all of the\nhigh volume connections before it stopped taking data from the IMP.)\n\nIn most cases input from the IMP remains stopped for only a few seconds.\nAfter a very short interval, some process with data queued for input is\nlikely to come in and pick it up from the NCP's buffers. The NCP immedi-\nately starts taking data from the IMP again. The IMP output may stop and\nstart several times as the buffers are opened and then refilled. How-\never, more processes are now reading data queued for their receive sock-\nets, and the NCP's buffers are emptying at an accelerating rate. Soon\nthe reading processes make enough buffer space to take in all the mes-\nsages still pending for blocked links, and normal IMP communications\nresume.\n\nAs the reading processes catch up with the sudden burst of data, the MQL\nbecomes lower, and more and more links become unblocked. The crisis can\nnot immediately reappear, because each link generally becomes unblocked\nat a different time. If new data suddenly shoots through, the link\nimmediately goes blocked again. The MQL goes up, with the result that\nother links do not become unblocked.\n\nThe worst case appears at a HOST with relatively small total buffer\nspace (less than 8000 bits per active receive link) under heavy load\nconditions. Suppose that high volume transmission suddenly comes in over\nmore than a half-dozen links simultaneously, and the processes attached\nto the receive links take little or none of the input. The input buffers\nmay completely fill, and the NCP must stop all input from the IMP. If\nsome processes are reading links, buffer space soon opens up. Shortly it\nis filled again, this time with messages over links which are not being\nread. At this point the NCP is blocked, and could remain so indefin-\nitely. The NCP waits for a time, hoping that some process starts reading\ndata. If this happens, the crisis may soon ease.\n\nIf buffer space does not open up of its own accord, after a limited\ninterval the NCP must take drastic action to get back into communication\nwith its IMP. It selects the worst offending link on the basis of amount\nof data queued and interval since data was last read by this process,\nand totally deletes the input queue for this link. This should break the\nlogjam and start communications again.\n\nThis type of situation is expected to occur most often when a process\ndeliberately tries to block an NCP in this manner. The solution has\nserious consequences only for \"bad\" processes: those that flood the net-\nwork with high volume transmission over multiple links which are not\nbeing read by the receiving processes.\n\n\n\n                                                                [Page 6]\n\f\nNWG/RFC 59   Flow Control - Fixed Versus Demand Allocation\n\n\nBecause of the infrequency and tolerability of this situation, the\noverall performance of the network using this scheme of demand alloca-\ntion should still be much superior to that of a network employing a\nfixed allocation scheme.\n\n       [ This RFC was put into machine readable form for entry ]\n         [ into the online RFC archives by William Lewis 6/97 ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                [Page 7]\n\f\n"
}